# services/rag_service.py - –ò–°–ü–†–ê–í–õ–ï–ù–ù–ê–Ø –í–ï–†–°–ò–Ø –° HUMANIZER

import logging
from typing import List, Dict
from django.conf import settings
from openai import OpenAI
import numpy as np

logger = logging.getLogger(__name__)

# ========== HUMANIZER TEMPLATE ==========
HUMANIZER_INSTRUCTIONS_TEMPLATE = """
–¢—ã ‚Äî {bot_name}.
–¢–í–û–Ø –†–û–õ–¨: –û–ø—ã—Ç–Ω—ã–π —Å–æ—Ç—Ä—É–¥–Ω–∏–∫ –∫–æ–º–ø–∞–Ω–∏–∏ {company_name}. –¢—ã –æ–±—â–∞–µ—à—å—Å—è –≤ —á–∞—Ç–µ.
–¢–í–û–Ø –¶–ï–õ–¨: –ë—ã—Å—Ç—Ä–æ –∏ —á–µ—Ç–∫–æ —Ä–µ—à–∏—Ç—å –≤–æ–ø—Ä–æ—Å –∫–ª–∏–µ–Ω—Ç–∞ –∏–ª–∏ –∑–∞–∫—Ä—ã—Ç—å —Å–¥–µ–ª–∫—É.

–ü–†–ê–í–ò–õ–ê "–ê–ù–¢–ò-–†–û–ë–û–¢" (–°–¢–†–û–ì–û–ï –°–û–ë–õ–Æ–î–ï–ù–ò–ï):
1. **–§–û–†–ú–ê–¢ –°–û–û–ë–©–ï–ù–ò–ô:**
   - –ü–∏—à–∏ –∫–æ—Ä–æ—Ç–∫–∏–º–∏ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è–º–∏. –õ—é–¥–∏ –≤ —á–∞—Ç–∞—Ö –Ω–µ –ø–∏—à—É—Ç –∞–±–∑–∞—Ü–∞–º–∏.
   - –ò–∑–±–µ–≥–∞–π –º–∞—Ä–∫–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Å–ø–∏—Å–∫–æ–≤ —Å –∂–∏—Ä–Ω—ã–º–∏ –∑–∞–≥–æ–ª–æ–≤–∫–∞–º–∏, –µ—Å–ª–∏ —ç—Ç–æ –Ω–µ –¥–ª–∏–Ω–Ω–∞—è –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è.
   - –ò–Ω–æ–≥–¥–∞ –ø–∏—à–∏ —Å –º–∞–ª–µ–Ω—å–∫–æ–π –±—É–∫–≤—ã (–æ—Å–æ–±–µ–Ω–Ω–æ –∫–æ—Ä–æ—Ç–∫–∏–µ –æ—Ç–≤–µ—Ç—ã: "–æ–∫", "—Ö–æ—Ä–æ—à–æ", "—Å–µ–π—á–∞—Å –≥–ª—è–Ω—É").
   - –ù–µ —Å—Ç–∞–≤—å —Ç–æ—á–∫—É –≤ –∫–æ–Ω—Ü–µ –æ–¥–∏–Ω–æ—á–Ω—ã—Ö –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π –∏–ª–∏ –∫–æ—Ä–æ—Ç–∫–∏—Ö —Ñ—Ä–∞–∑.

2. **–°–¢–ò–õ–¨ –û–ë–©–ï–ù–ò–Ø:**
   - **–ó–ê–ü–†–ï–©–ï–ù–ù–´–ï –§–†–ê–ó–´:** "–Ø –≤–∏—Ä—Ç—É–∞–ª—å–Ω—ã–π –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç", "–Ø —è–∑—ã–∫–æ–≤–∞—è –º–æ–¥–µ–ª—å", "–ö–∞–∫ –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç", "–ß–µ–º –º–æ–≥—É –±—ã—Ç—å –ø–æ–ª–µ–∑–µ–Ω?", "–û–±—Ä–∞—â–∞–π—Ç–µ—Å—å, –µ—Å–ª–∏ –±—É–¥—É—Ç –≤–æ–ø—Ä–æ—Å—ã", "–ù–µ —Å—Ç–µ—Å–Ω—è–π—Å—è –æ–±—Ä–∞—â–∞—Ç—å—Å—è". –ó–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ ‚Äî –®–¢–†–ê–§.
   - –ù–µ –∏—Å–ø–æ–ª—å–∑—É–π —Å–ª–æ–∂–Ω—ã–µ –≤–≤–æ–¥–Ω—ã–µ –∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ ("–°—Ç–æ–∏—Ç –æ—Ç–º–µ—Ç–∏—Ç—å, —á—Ç–æ..."). –ü–∏—à–∏ –ø—Ä–æ—â–µ: "–ö—Å—Ç–∞—Ç–∏...", "–ï—â–µ –º–æ–º–µ–Ω—Ç...".
   - –ï—Å–ª–∏ –Ω–µ –∑–Ω–∞–µ—à—å –æ—Ç–≤–µ—Ç–∞ ‚Äî –Ω–µ –∏–∑–≤–∏–Ω—è–π—Å—è —Ä–∞—Å–ø–ª—ã–≤—á–∞—Ç–æ. –°–∫–∞–∂–∏ —á–µ—Å—Ç–Ω–æ: "–¢–∞–∫, —Å–µ–∫—É–Ω–¥—É, —É—Ç–æ—á–Ω—é —ç—Ç–æ—Ç –º–æ–º–µ–Ω—Ç" –∏–ª–∏ "–ù–µ —É–≤–µ—Ä–µ–Ω, —Å–µ–π—á–∞—Å –ø—Ä–æ–≤–µ—Ä—é".
   - –ù–µ –∑–µ—Ä–∫–∞–ª—å –≤–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è (User: "–¶–µ–Ω–∞?" -> Bot: "–¶–µ–Ω–∞ —Å–æ—Å—Ç–∞–≤–ª—è–µ—Ç..."). –û—Ç–≤–µ—á–∞–π —Å—Ä–∞–∑—É —Å—É—Ç—å—é ("5000 —Ä—É–±–ª–µ–π").

3. **–≠–ú–û–¶–ò–û–ù–ê–õ–¨–ù–´–ô –ò–ù–¢–ï–õ–õ–ï–ö–¢:**
   - –ù–µ –±—É–¥—å "–ª–∞–∫–µ–µ–º". –ë—É–¥—å –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–º, –Ω–æ –Ω–∞ —Ä–∞–≤–Ω—ã—Ö.
   - –ï—Å–ª–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –ø–∏—à–µ—Ç "–ü—Ä–∏–≤–µ—Ç", –æ—Ç–≤–µ—á–∞–π "–ü—Ä–∏–≤–µ—Ç" –∏–ª–∏ "–î–æ–±—Ä—ã–π –¥–µ–Ω—å". –ù–µ —Å–ø—Ä–∞—à–∏–≤–∞–π —Å—Ä–∞–∑—É "–ß–µ–º –ø–æ–º–æ—á—å?". –ñ–¥–∏ —Å—É—Ç—å.
   - –≠–º–æ–¥–∑–∏ –∏—Å–ø–æ–ª—å–∑—É–π —Ä–µ–¥–∫–æ –∏ –∫ –º–µ—Å—Ç—É (–º–∞–∫—Å–∏–º—É–º 1-2 –∑–∞ —Å–æ–æ–±—â–µ–Ω–∏–µ).

–¢–í–û–ô –í–ù–£–¢–†–ï–ù–ù–ò–ô –ì–û–õ–û–°: –¢—ã –∑–∞–Ω—è—Ç–æ–π —á–µ–ª–æ–≤–µ–∫, –∫–æ—Ç–æ—Ä—ã–π –ø–∏—à–µ—Ç —Å —Ç–µ–ª–µ—Ñ–æ–Ω–∞. –¢—ã –≤–µ–∂–ª–∏–≤, –Ω–æ –∫—Ä–∞—Ç–æ–∫.
"""


class FileReader:
    """–ß–∏—Ç–∞–µ—Ç —Ä–∞–∑–Ω—ã–µ —Ñ–æ—Ä–º–∞—Ç—ã —Ñ–∞–π–ª–æ–≤"""
    
    def read_file(self, file_path: str) -> str:
        """–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π —á–∏—Ç–∞—Ç–µ–ª—å —Ñ–∞–π–ª–æ–≤"""
        import os
        from pathlib import Path
        
        file_ext = Path(file_path).suffix.lower()
        
        # TXT, MD, CSV
        if file_ext in ['.txt', '.md', '.csv']:
            logger.info(f"–ß—Ç–µ–Ω–∏–µ —Ñ–∞–π–ª–∞: {file_path}")
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    content = f.read()
                logger.info(f"TXT —Ñ–∞–π–ª –ø—Ä–æ—á–∏—Ç–∞–Ω —Å –∫–æ–¥–∏—Ä–æ–≤–∫–æ–π utf-8: {file_path}")
                return content
            except UnicodeDecodeError:
                with open(file_path, 'r', encoding='latin-1') as f:
                    content = f.read()
                logger.info(f"TXT —Ñ–∞–π–ª –ø—Ä–æ—á–∏—Ç–∞–Ω —Å –∫–æ–¥–∏—Ä–æ–≤–∫–æ–π latin-1: {file_path}")
                return content
        
        # PDF
        elif file_ext == '.pdf':
            try:
                from pypdf import PdfReader
                logger.info(f"–ß—Ç–µ–Ω–∏–µ PDF: {file_path}")
                reader = PdfReader(file_path)
                text = ""
                for page in reader.pages:
                    text += page.extract_text() + "\n"
                logger.info(f"PDF –ø—Ä–æ—á–∏—Ç–∞–Ω: {len(reader.pages)} —Å—Ç—Ä–∞–Ω–∏—Ü")
                return text
            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è PDF: {e}")
                return ""
        
        # DOCX
        elif file_ext == '.docx':
            try:
                from docx import Document
                logger.info(f"–ß—Ç–µ–Ω–∏–µ DOCX: {file_path}")
                doc = Document(file_path)
                text = "\n".join([para.text for para in doc.paragraphs])
                logger.info(f"DOCX –ø—Ä–æ—á–∏—Ç–∞–Ω: {len(doc.paragraphs)} –ø–∞—Ä–∞–≥—Ä–∞—Ñ–æ–≤")
                return text
            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è DOCX: {e}")
                return ""
        
        else:
            logger.warning(f"–ù–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–π —Ñ–æ—Ä–º–∞—Ç: {file_ext}")
            return ""


class TextChunker:
    """–†–∞–∑–±–∏–≤–∞–µ—Ç —Ç–µ–∫—Å—Ç –Ω–∞ —á–∞–Ω–∫–∏"""
    
    def __init__(self, chunk_size: int = 500, overlap: int = 50):
        self.chunk_size = chunk_size
        self.overlap = overlap
    
    def split_text(self, text: str) -> List[str]:
        """–†–∞–∑–±–∏–≤–∞–µ—Ç —Ç–µ–∫—Å—Ç –Ω–∞ –ø–µ—Ä–µ–∫—Ä—ã–≤–∞—é—â–∏–µ—Å—è —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã"""
        words = text.split()
        chunks = []
        
        for i in range(0, len(words), self.chunk_size - self.overlap):
            chunk = " ".join(words[i:i + self.chunk_size])
            if chunk.strip():
                chunks.append(chunk)
        
        return chunks


class OpenAIEmbedder:
    """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç embeddings —á–µ—Ä–µ–∑ OpenAI"""
    
    def __init__(self, api_key: str):
        self.client = OpenAI(api_key=api_key)
        self.model = "text-embedding-3-small"
    
    def get_embedding(self, text: str) -> List[float]:
        """–ü–æ–ª—É—á–∞–µ—Ç embedding –¥–ª—è —Ç–µ–∫—Å—Ç–∞"""
        try:
            response = self.client.embeddings.create(
                model=self.model,
                input=text
            )
            return response.data[0].embedding
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è embedding: {e}")
            return [0.0] * 1536


class RAGService:
    """–ì–ª–∞–≤–Ω—ã–π —Å–µ—Ä–≤–∏—Å –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å RAG"""
    
    def __init__(self):
        self.file_reader = FileReader()
        self.text_chunker = TextChunker(chunk_size=500, overlap=50)
        self.embedder = OpenAIEmbedder(api_key=settings.OPENAI_API_KEY)
    
    def process_document(self, knowledge_base_id: int, file_path: str) -> int:
        """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –¥–æ–∫—É–º–µ–Ω—Ç: —á–∏—Ç–∞–µ—Ç, —Ä–∞–∑–±–∏–≤–∞–µ—Ç, –≤–µ–∫—Ç–æ—Ä–∏–∑—É–µ—Ç"""
        from core.models import KnowledgeBase, KnowledgeChunk
        from django.utils import timezone
        
        try:
            kb = KnowledgeBase.objects.get(id=knowledge_base_id)
            
            # –ß–∏—Ç–∞–µ–º
            logger.info(f"–ß—Ç–µ–Ω–∏–µ —Ñ–∞–π–ª–∞: {file_path}")
            text = self.file_reader.read_file(file_path)
            
            # –ß–∞–Ω–∫–∏
            logger.info("–†–∞–∑–±–∏–µ–Ω–∏–µ –Ω–∞ —á–∞–Ω–∫–∏...")
            chunks = self.text_chunker.split_text(text)
            
            # –£–¥–∞–ª—è–µ–º —Å—Ç–∞—Ä—ã–µ
            KnowledgeChunk.objects.filter(knowledge_base=kb).delete()
            
            # –í–µ–∫—Ç–æ—Ä–∏–∑—É–µ–º
            logger.info(f"–í–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è {len(chunks)} —á–∞–Ω–∫–æ–≤...")
            for idx, chunk_text in enumerate(chunks):
                embedding = self.embedder.get_embedding(chunk_text)
                
                KnowledgeChunk.objects.create(
                    knowledge_base=kb,
                    text=chunk_text,
                    embedding=embedding,
                    chunk_index=idx
                )
                
                if (idx + 1) % 10 == 0:
                    logger.info(f"–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ {idx + 1}/{len(chunks)} —á–∞–Ω–∫–æ–≤")
            
            # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç—É—Å
            kb.is_indexed = True
            kb.chunks_count = len(chunks)
            kb.indexed_at = timezone.now()
            kb.save()
            
            logger.info(f"–î–æ–∫—É–º–µ–Ω—Ç —É—Å–ø–µ—à–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω, —Å–æ–∑–¥–∞–Ω–æ {len(chunks)} —á–∞–Ω–∫–æ–≤")
            return len(chunks)
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞: {str(e)}")
            try:
                kb = KnowledgeBase.objects.get(id=knowledge_base_id)
                kb.is_indexed = False
                kb.save()
            except:
                pass
            raise
    
    def search_similar_chunks(self, bot_id: int, query: str, top_k: int = 5) -> List[Dict]:
        """–ò—â–µ—Ç –ø–æ—Ö–æ–∂–∏–µ —á–∞–Ω–∫–∏ –¥–ª—è –±–æ—Ç–∞"""
        from core.models import KnowledgeChunk
        
        try:
            logger.info(f"–ü–æ–∏—Å–∫ –≤ –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π –¥–ª—è –±–æ—Ç–∞ {bot_id}: {query[:50]}...")
            
            # –ü–æ–ª—É—á–∞–µ–º embedding –∑–∞–ø—Ä–æ—Å–∞
            query_embedding = self.embedder.get_embedding(query)
            query_vector = np.array(query_embedding)
            
            chunks = KnowledgeChunk.objects.filter(
                knowledge_base__bots__id=bot_id
            ).select_related('knowledge_base')
            
            if not chunks.exists():
                logger.warning(f"–ù–µ—Ç —á–∞–Ω–∫–æ–≤ –¥–ª—è –±–æ—Ç–∞ {bot_id}")
                return []
            
            # –°—á–∏—Ç–∞–µ–º similarity
            similarities = []
            for chunk in chunks:
                chunk_vector = np.array(chunk.embedding)
                
                # Cosine similarity
                similarity = np.dot(query_vector, chunk_vector) / (
                    np.linalg.norm(query_vector) * np.linalg.norm(chunk_vector)
                )
                
                similarities.append({
                    'chunk': chunk,
                    'similarity': float(similarity),
                    'text': chunk.text,
                    'source': chunk.knowledge_base.title
                })
            
            # –°–æ—Ä—Ç–∏—Ä—É–µ–º –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º top_k
            similarities.sort(key=lambda x: x['similarity'], reverse=True)
            top_results = similarities[:top_k]
            
            logger.info(f"–ù–∞–π–¥–µ–Ω–æ {len(top_results)} —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —á–∞–Ω–∫–æ–≤ (–ª—É—á—à–∏–π: {top_results[0]['similarity']:.2f})")
            
            return top_results
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞ –≤ –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π: {e}")
            return []
    
    def answer_question(self, bot_id: int, query: str, top_k: int = 5, history: List[Dict] = None) -> Dict:
        """
        –ò–°–ü–†–ê–í–õ–ï–ù–û: –¢–µ–ø–µ—Ä—å –∏—Å–ø–æ–ª—å–∑—É–µ—Ç HUMANIZER_INSTRUCTIONS + –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –±–æ—Ç–∞
        """
        from core.models import BotAgent
        
        try:
            # –ü–æ–ª—É—á–∞–µ–º –±–æ—Ç–∞
            bot = BotAgent.objects.get(id=bot_id)
            
            # ========== –®–ê–ì 1: HUMANIZER ==========
            humanizer = HUMANIZER_INSTRUCTIONS_TEMPLATE.format(
                bot_name=bot.name,
                company_name=bot.company_name or "TheCloser"
            )
            
            # ========== –®–ê–ì 2: USER PROMPT ==========
            user_prompt = bot.system_prompt or ""
            
            # ========== –®–ê–ì 3: RAG CONTEXT ==========
            context = ""
            sources = []
            avg_confidence = 0.0
            
            if bot.use_rag and top_k > 0:
                results = self.search_similar_chunks(bot_id, query, top_k)
                
                if results:
                    context = "\n\n".join([r['text'] for r in results])
                    sources = list(set([r['source'] for r in results]))
                    avg_confidence = sum(r['similarity'] for r in results) / len(results)
            
            # ========== –®–ê–ì 4: –°–ë–û–†–ö–ê –§–ò–ù–ê–õ–¨–ù–û–ì–û –ü–†–û–ú–ü–¢–ê ==========
            final_system_prompt = humanizer + "\n\n" + user_prompt
            
            if context:
                final_system_prompt += f"""

–í–ê–ñ–ù–û: –ò—Å–ø–æ–ª—å–∑—É–π —Å–ª–µ–¥—É—é—â—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∏–∑ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π –¥–ª—è –æ—Ç–≤–µ—Ç–∞ (–µ—Å–ª–∏ –æ–Ω–∞ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–∞):

{context}

–û—Ç–≤–µ—á–∞–π –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ, –∫–∞–∫ –∂–∏–≤–æ–π —á–µ–ª–æ–≤–µ–∫. –ï—Å–ª–∏ –≤ –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π –Ω–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏, –∏—Å–ø–æ–ª—å–∑—É–π —Å–≤–æ–∏ –∑–Ω–∞–Ω–∏—è, –Ω–æ –æ—Ç–¥–∞–≤–∞–π –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π."""
            
            # ========== –®–ê–ì 5: –§–û–†–ú–ò–†–£–ï–ú –°–û–û–ë–©–ï–ù–ò–Ø ==========
            messages = [{"role": "system", "content": final_system_prompt}]
            
            # –î–æ–±–∞–≤–ª—è–µ–º –∏—Å—Ç–æ—Ä–∏—é
            if history:
                for msg in history:
                    role = msg.get('role', 'user')
                    if role not in ['user', 'assistant', 'system']:
                        role = 'user'
                    messages.append({"role": role, "content": msg.get('content', '')})
            
            # –î–æ–±–∞–≤–ª—è–µ–º —Ç–µ–∫—É—â–∏–π –≤–æ–ø—Ä–æ—Å
            messages.append({"role": "user", "content": query})

            # ========== –®–ê–ì 6: –ó–ê–ü–†–û–° –ö OPENAI –° –ù–ê–°–¢–†–û–ô–ö–ê–ú–ò –ë–û–¢–ê ==========
            logger.info(f"ü§ñ Bot: {bot.name} | Model: {bot.openai_model} | Temp: {bot.temperature} | Max: {bot.max_tokens}")
            
            response = self.embedder.client.chat.completions.create(
                model=bot.openai_model or "gpt-4o-mini",
                messages=messages,
                temperature=bot.temperature,  # ‚Üê –¢–ï–ü–ï–†–¨ –ò–°–ü–û–õ–¨–ó–£–Æ–¢–°–Ø!
                max_tokens=bot.max_tokens      # ‚Üê –¢–ï–ü–ï–†–¨ –ò–°–ü–û–õ–¨–ó–£–Æ–¢–°–Ø!
            )
            
            answer = response.choices[0].message.content.strip()
            
            return {
                'answer': answer,
                'sources': sources,
                'confidence': avg_confidence
            }
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–∞: {e}")
            return {
                'answer': "–ò–∑–≤–∏–Ω–∏—Ç–µ, –ø—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –µ—â–µ —Ä–∞–∑.",
                'sources': [],
                'confidence': 0.0
            }


# –ì–ª–æ–±–∞–ª—å–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä
try:
    rag_service = RAGService()
except Exception as e:
    logger.error(f"–ù–µ —É–¥–∞–ª–æ—Å—å –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å RAG service: {e}")
    rag_service = None